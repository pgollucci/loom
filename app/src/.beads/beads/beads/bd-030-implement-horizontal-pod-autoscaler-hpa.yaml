entitymetadata:
    schemaversion: ""
    attributes: {}
    migratedat: null
    migratedfrom: ""
id: bd-030
type: feature
title: Implement Horizontal Pod Autoscaler (HPA)
description: |-
    Enable auto-scaling based on CPU and custom metrics.

    Benefits:
    - Automatic scaling during load spikes
    - Cost optimization during low traffic
    - Improved availability

    Implementation:
    1. Ensure resource requests are defined (prerequisite)
    2. Deploy metrics-server in cluster
    3. Create k8s/hpa.yaml with HPA config
    4. Configure scaling based on CPU (70% target)
    5. Add custom metric: agenticorp_beads_queue_length
    6. Set minReplicas: 2, maxReplicas: 10
    7. Test scaling behavior under load
    8. Monitor scaling events

    Deliverables:
    - HPA manifest
    - Load testing showing scale-up/down
    - Documentation of scaling behavior
status: in_progress
priority: 2
projectid: agenticorp-self
assignedto: agent-1769138560-Web Designer (Default)
blockedby: []
blocks: []
relatedto: []
parent: ""
children: []
tags: []
context:
    agent_id: agent-1769138560-Web Designer (Default)
    agent_output: |-
        We need to produce a response in the specified format, following the persona, etc. We need to work on bead bd-030: Implement Horizontal Pod Autoscaler (HPA). Provide short plan, key questions/risks, concrete next actions, proposed patch snippets. Must consider autonomy: this is a feature implementation likely within autonomy? It's a feature but maybe not major layout restructuring; it's about adding HPA manifest, custom metric, etc. Probably within autonomy, but need to check if any decision bead needed. It's a P2 priority. Should be doable autonomously. However need to ensure resource requests are defined (prerequisite). Might need to check if that is done. Could be a decision bead if not. But we can proceed.

        We should output in plain text, not code block. The output format: 1) Short plan, 2) Key questions/risks, 3) Concrete next actions (commands/files to change), 4) Proposed patch snippets if applicable.

        We should not use markdown formatting unless asked; the instruction says avoid formatting unless explicitly asked. So we should output plain text, maybe with headings but not markdown syntax? The example uses headings with # but that's markdown. The instruction says avoid formatting (bold, italic, underline, markdown) unless explicitly asked. So we should not use markdown headings. We can just use plain text lines.

        Thus produce something like:

        Short plan:
        - Review current deployments for resource requests...
        - Ensure metrics-server installed...
        - Create k8s/hpa.yaml...
        - Apply manifest...
        - Test scaling...

        Key questions/risks:
        - Are resource requests defined?
        - Custom metric availability...
        - Load testing environment...

        Concrete next actions:
        - Run command to check deployments...
        - Install metrics-server...
        - Create file k8s/hpa.yaml with content...
        - Apply with kubectl apply -f ...
        - Run load test script...
        - Verify HPA status...

        Proposed patch snippets:
        - Provide sample HPA manifest snippet.

        Make sure no markdown formatting. Use plain text.

        Let's craft.
        </think>
        Short plan
        - Verify that all pods in the current deployment have CPU requests defined
        - Confirm the metrics-server add‑on is installed in the cluster
        - Write a k8s/hpa.yaml manifest that references CPU target (70%) and the custom metric agenticorp_beads_queue_length
        - Apply the manifest and check the HPA status
        - Run load‑testing scripts to provoke scale‑up and scale‑down events
        - Document the scaling behavior and update any relevant documentation

        Key questions/risks
        - Are CPU requests present on every deployment? If not, that must be fixed first.
        - Is the custom metric agenticorp_beads_queue_length exposed by the Prometheus adapter or another metrics source?
        - Will the chosen minReplicas (2) and maxReplicas (10) cause unexpected churn during brief traffic spikes?
        - Does the cluster have sufficient quota to accommodate the additional replicas?
        - Load‑testing may affect production‑like environments; ensure tests are isolated.

        Concrete next actions
        - List current deployments and inspect their resource.requests.cpu fields
        - Install metrics-server if not already present: helm install metrics-server metrics-server/metrics-server --namespace kube-system --set apiService.enabled=true
        - Create or edit k8s/hpa.yaml with the following structure (see patch snippet below)
        - Apply the manifest: kubectl apply -f k8s/hpa.yaml
        - Check HPA status: kubectl get hpa -n <namespace>
        - Execute the load‑testing tool (e.g., ./scripts/run_load_test.sh) to generate CPU and queue pressure
        - Observe scaling events with: kubectl describe hpa -n <namespace>
        - Record the scaling metrics and add notes to the documentation folder

        Proposed patch snippet for k8s/hpa.yaml
        ```
        apiVersion: autoscaling/v2
        kind: HorizontalPodAutoscaler
        metadata:
          name: agenticorp-self-hpa
          namespace: default
        spec:
          scaleTargetRef:
            apiVersion: apps/v1
            kind: Deployment
            name: agenticorp-self
          minReplicas: 2
          maxReplicas: 10
          metrics:
          - type: Resource
            resource:
              name: cpu
              target:
                type: Utilization
                averageUtilization: 70
          - type: Pods
            pods:
              metric:
                name: agenticorp_beads_queue_length
              target:
                type: AverageValue
                averageValue: 50m
        ```
    agent_task_id: task-bd-030-1769146428772574136
    agent_tokens: "1842"
    agent_worker_id: worker-agent-1769138560-Web Designer (Default)-1769146419
    dispatch_history: '["agent-1769138560-Web Designer (Default)"]'
    last_run_at: "2026-01-23T05:34:09Z"
    loop_detected: "false"
    provider_id: sparky-vllm
    provider_model: nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8
    redispatch_requested: "false"
createdat: 2026-01-23T00:08:10.911404758Z
updatedat: 2026-01-23T05:34:09.554063215Z
closedat: null
