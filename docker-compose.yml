services:
  # PostgreSQL database for Temporal
  temporal-postgresql:
    image: postgres:15-alpine
    container_name: temporal-postgresql
    restart: unless-stopped
    environment:
      POSTGRES_USER: temporal
      POSTGRES_PASSWORD: temporal
      POSTGRES_DB: temporal
    networks:
      - loom-network
    volumes:
      - temporal-db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U temporal"]
      interval: 10s
      timeout: 5s
      retries: 5

  # PostgreSQL database for Loom
  loom-postgresql:
    image: postgres:15-alpine
    container_name: loom-postgresql
    restart: unless-stopped
    environment:
      POSTGRES_USER: loom
      POSTGRES_PASSWORD: loom
      POSTGRES_DB: loom
    networks:
      - loom-network
    volumes:
      - loom-db-postgres:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U loom"]
      interval: 10s
      timeout: 5s
      retries: 5
    ports:
      - "5432:5432"  # Expose for development/debugging

  # Temporal server with auto-setup
  temporal:
    image: temporalio/auto-setup:1.22.4
    container_name: temporal
    restart: unless-stopped
    depends_on:
      temporal-postgresql:
        condition: service_healthy
    environment:
      - DB=postgresql
      - DB_PORT=5432
      - POSTGRES_USER=temporal
      - POSTGRES_PWD=temporal
      - POSTGRES_SEEDS=temporal-postgresql
      - DYNAMIC_CONFIG_FILE_PATH=config/dynamicconfig/development-sql.yaml
      - ENABLE_ES=false
      - ES_SEEDS=
      - ES_VERSION=
    ports:
      - "7233:7233"  # gRPC port
    networks:
      - loom-network
    volumes:
      - ./config/temporal:/etc/temporal/config/dynamicconfig
    healthcheck:
      test: ["CMD", "tctl", "--address", "temporal:7233", "cluster", "health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Temporal Web UI
  temporal-ui:
    image: temporalio/ui:2.21.3
    container_name: temporal-ui
    restart: unless-stopped
    depends_on:
      temporal:
        condition: service_healthy
    environment:
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_CORS_ORIGINS=http://localhost:3000
    ports:
      - "8088:8080"  # Temporal UI on different port to avoid conflict with loom
    networks:
      - loom-network

  # TokenHub: LLM routing proxy with intelligent provider selection,
  # health tracking, failover, and Thompson Sampling.
  # All Loom agents route LLM requests through TokenHub instead of
  # directly hitting provider endpoints.
  tokenhub:
    build:
      context: ../tokenhub
      dockerfile: Dockerfile.slim
    image: tokenhub:latest
    container_name: tokenhub
    restart: unless-stopped
    environment:
      - TOKENHUB_LISTEN_ADDR=:8080
      - TOKENHUB_LOG_LEVEL=info
      - TOKENHUB_DB_DSN=file:/data/tokenhub.sqlite?_pragma=busy_timeout(5000)&_pragma=journal_mode(WAL)
      - TOKENHUB_VAULT_ENABLED=false
      # vLLM endpoints (sparky local): round-robin across both
      - TOKENHUB_VLLM_ENDPOINTS=http://sparky.local:8000/v1,http://sparky.local:8001/v1
      # Additional OpenAI-compatible providers (NVIDIA NIM cloud)
      - TOKENHUB_EXTRA_PROVIDERS=${TOKENHUB_EXTRA_PROVIDERS:-}
      # Disable health probes (providers use auth that probes can't supply)
      - TOKENHUB_HEALTH_PROBE_DISABLED=true
      # Routing policy: balanced selection, generous budget/latency for agent workloads
      - TOKENHUB_DEFAULT_MODE=normal
      - TOKENHUB_DEFAULT_MAX_BUDGET_USD=1.00
      - TOKENHUB_DEFAULT_MAX_LATENCY_MS=120000
      - TOKENHUB_PROVIDER_TIMEOUT_SECS=120
      # Security: admin token for provider/model registration API
      - TOKENHUB_ADMIN_TOKEN=${TOKENHUB_ADMIN_TOKEN:-loom-tokenhub-admin}
      - TOKENHUB_CORS_ORIGINS=*
      - TOKENHUB_RATE_LIMIT_RPS=120
      - TOKENHUB_RATE_LIMIT_BURST=240
      # Temporal disabled — Loom has its own Temporal
      - TOKENHUB_TEMPORAL_ENABLED=false
    extra_hosts:
      - "sparky.local:10.11.100.116"
    networks:
      - loom-network
    volumes:
      - tokenhub-data:/data
    ports:
      - "8090:8080"  # TokenHub admin UI on 8090 to avoid conflict with Loom on 8080
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/healthz"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s

  # NATS message bus with JetStream for inter-service communication
  nats:
    image: nats:2.10-alpine
    container_name: nats
    restart: unless-stopped
    command:
      - "-js"
      - "-sd"
      - "/data"
      - "-m"
      - "8222"
    ports:
      - "4222:4222"  # Client connections
      - "8222:8222"  # HTTP management/monitoring
      - "6222:6222"  # Cluster connections
    networks:
      - loom-network
    volumes:
      - nats-data:/data
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8222/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5

  # PgBouncer connection pooler for loom-postgresql
  pgbouncer:
    image: edoburu/pgbouncer:latest
    container_name: pgbouncer
    restart: unless-stopped
    depends_on:
      loom-postgresql:
        condition: service_healthy
    environment:
      - DB_USER=loom
      - DB_PASSWORD=loom
      - DB_HOST=loom-postgresql
      - DB_PORT=5432
      - DB_NAME=loom
      - POOL_MODE=session
      - MAX_CLIENT_CONN=100
      - DEFAULT_POOL_SIZE=20
      - SERVER_RESET_QUERY=DISCARD ALL
      - IGNORE_STARTUP_PARAMETERS=extra_float_digits
    ports:
      - "5433:5432"  # Expose on 5433 to avoid conflict with loom-postgresql:5432
    networks:
      - loom-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost -U loom"]
      interval: 10s
      timeout: 5s
      retries: 5

  loom:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        GITHUB_TOKEN: ${GITHUB_TOKEN}
    image: loom:latest
    container_name: loom
    restart: unless-stopped
    depends_on:
      temporal:
        condition: service_healthy
      nats:
        condition: service_healthy
      pgbouncer:
        condition: service_healthy
      tokenhub:
        condition: service_healthy
    ports:
      - "8080:8081"
      - "3307:3307"
    environment:
      - LOOM_PASSWORD=${LOOM_PASSWORD}
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - GITLAB_TOKEN=${GITLAB_TOKEN}
      - TZ=UTC
      - TEMPORAL_HOST=temporal:7233
      - TEMPORAL_NAMESPACE=default
      - NATS_URL=nats://nats:4222
      - CONFIG_PATH=/app/config.yaml
      - DOLT_PORT=3307
      - DB_TYPE=postgres
      - POSTGRES_HOST=pgbouncer
      - POSTGRES_PORT=5432
      - POSTGRES_USER=loom
      - POSTGRES_PASSWORD=loom
      - POSTGRES_DB=loom
      - CONNECTORS_SERVICE_ADDR=connectors-service:50051
      - OTEL_ENDPOINT=otel-collector:4317
      - TOKENHUB_URL=http://tokenhub:8080
      - TOKENHUB_ADMIN_TOKEN=${TOKENHUB_ADMIN_TOKEN:-loom-tokenhub-admin}
    networks:
      - loom-network
    extra_hosts:
      - "plubbit.local:10.11.101.250"
      - "sparky.local:10.11.100.116"
    volumes:
      # Persist SQLite database and project clones across container restarts
      - loom-db:/app/data
      # Mount known_hosts for git SSH operations (read-only)
      # Loom uses its own per-project SSH deploy keys (stored encrypted in DB)
      - ~/.ssh/known_hosts:/home/loom/.ssh/known_hosts:ro
      # Mount Docker socket for per-project container orchestration
      - /var/run/docker.sock:/var/run/docker.sock
    healthcheck:
      test: ["CMD", "/app/loom", "-version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    labels:
      linkerd.io/inject: enabled

  # Agent Service: Coder -- subscribes to loom.tasks.*.coder
  loom-agent-coder:
    build:
      context: .
      dockerfile: Dockerfile
    image: loom:latest
    restart: unless-stopped
    depends_on:
      nats:
        condition: service_healthy
      loom:
        condition: service_healthy
    environment:
      - PROJECT_ID=${PROJECT_ID:-loom}
      - CONTROL_PLANE_URL=http://loom:8081
      - NATS_URL=nats://nats:4222
      - AGENT_ROLE=coder
      - PROVIDER_ENDPOINT=${PROVIDER_ENDPOINT}
      - PROVIDER_MODEL=${PROVIDER_MODEL:-qwen3-235b-a22b}
      - PROVIDER_API_KEY=${PROVIDER_API_KEY}
      - ACTION_LOOP_ENABLED=true
      - MAX_LOOP_ITERATIONS=25
      - WORK_DIR=/workspace
      - OTEL_ENDPOINT=otel-collector:4317
    networks:
      - loom-network
    volumes:
      - agent-coder-workspace:/workspace
    entrypoint: ["/app/loom-project-agent"]
    labels:
      linkerd.io/inject: enabled
    deploy:
      replicas: ${CODER_REPLICAS:-1}

  # Agent Service: Reviewer -- subscribes to loom.reviews.*
  loom-agent-reviewer:
    build:
      context: .
      dockerfile: Dockerfile
    image: loom:latest
    restart: unless-stopped
    depends_on:
      nats:
        condition: service_healthy
      loom:
        condition: service_healthy
    environment:
      - PROJECT_ID=${PROJECT_ID:-loom}
      - CONTROL_PLANE_URL=http://loom:8081
      - NATS_URL=nats://nats:4222
      - AGENT_ROLE=reviewer
      - PROVIDER_ENDPOINT=${PROVIDER_ENDPOINT}
      - PROVIDER_MODEL=${PROVIDER_MODEL:-qwen3-235b-a22b}
      - PROVIDER_API_KEY=${PROVIDER_API_KEY}
      - ACTION_LOOP_ENABLED=true
      - MAX_LOOP_ITERATIONS=15
      - WORK_DIR=/workspace
      - OTEL_ENDPOINT=otel-collector:4317
    networks:
      - loom-network
    volumes:
      - agent-reviewer-workspace:/workspace
    entrypoint: ["/app/loom-project-agent"]
    labels:
      linkerd.io/inject: enabled
    deploy:
      replicas: ${REVIEWER_REPLICAS:-1}

  # Agent Service: QA -- subscribes to loom.tasks.*.qa
  loom-agent-qa:
    build:
      context: .
      dockerfile: Dockerfile
    image: loom:latest
    restart: unless-stopped
    depends_on:
      nats:
        condition: service_healthy
      loom:
        condition: service_healthy
    environment:
      - PROJECT_ID=${PROJECT_ID:-loom}
      - CONTROL_PLANE_URL=http://loom:8081
      - NATS_URL=nats://nats:4222
      - AGENT_ROLE=qa
      - PROVIDER_ENDPOINT=${PROVIDER_ENDPOINT}
      - PROVIDER_MODEL=${PROVIDER_MODEL:-qwen3-235b-a22b}
      - PROVIDER_API_KEY=${PROVIDER_API_KEY}
      - ACTION_LOOP_ENABLED=true
      - MAX_LOOP_ITERATIONS=10
      - WORK_DIR=/workspace
      - OTEL_ENDPOINT=otel-collector:4317
    networks:
      - loom-network
    volumes:
      - agent-qa-workspace:/workspace
    entrypoint: ["/app/loom-project-agent"]
    labels:
      linkerd.io/inject: enabled
    deploy:
      replicas: ${QA_REPLICAS:-1}

  # Connectors microservice — manages external service integrations via gRPC
  connectors-service:
    build:
      context: .
      dockerfile: Dockerfile
    image: loom:latest
    container_name: connectors-service
    restart: unless-stopped
    depends_on:
      nats:
        condition: service_healthy
    environment:
      - GRPC_PORT=50051
      - CONFIG_PATH=/app/config/connectors.yaml
      - HEALTH_INTERVAL=30s
      - OTEL_ENDPOINT=otel-collector:4317
    ports:
      - "50051:50051"  # gRPC
    networks:
      - loom-network
    volumes:
      - ./config:/app/config
    entrypoint: ["/app/connectors-service"]
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://localhost:50051 || true"]
      interval: 15s
      timeout: 5s
      retries: 3
    labels:
      linkerd.io/inject: enabled

  loom-test:
    build:
      context: .
      dockerfile: Dockerfile
      target: builder
      args:
        GITHUB_TOKEN: ${GITHUB_TOKEN}
    container_name: loom-test
    depends_on:
      temporal:
        condition: service_healthy
    environment:
      - TEMPORAL_HOST=temporal:7233
      - TEMPORAL_REQUIRED=true
    networks:
      - loom-network
    command: ["go", "test", "-v", "./..."]

  # OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    container_name: otel-collector
    restart: unless-stopped
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./config/otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "8888:8888"   # Prometheus metrics exposed by the collector
      - "8889:8889"   # Prometheus exporter
    networks:
      - loom-network

  # Prometheus for metrics storage
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - loom-network

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - grafana-data:/var/lib/grafana
    ports:
      - "3000:3000"
    networks:
      - loom-network
    depends_on:
      - prometheus

  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    restart: unless-stopped
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "16686:16686"  # Jaeger UI
      - "14268:14268"  # Jaeger collector HTTP
    networks:
      - loom-network

  # Loki for log aggregation
  loki:
    image: grafana/loki:latest
    container_name: loki
    restart: unless-stopped
    command: -config.file=/etc/loki/local-config.yaml
    ports:
      - "3100:3100"  # Loki API
    networks:
      - loom-network
    volumes:
      - ./config/loki:/etc/loki
      - loki-data:/loki

  # Promtail for log forwarding
  promtail:
    image: grafana/promtail:latest
    container_name: promtail
    restart: unless-stopped
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - ./config/promtail:/etc/promtail
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - loom-network
    depends_on:
      - loki

networks:
  loom-network:
    driver: bridge

volumes:
  temporal-db-data:
  loom-db:
  loom-db-postgres:
  nats-data:
  prometheus-data:
  grafana-data:
  loki-data:
  tokenhub-data:
  agent-coder-workspace:
  agent-reviewer-workspace:
  agent-qa-workspace:
