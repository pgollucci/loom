services:
  # PostgreSQL database for Loom
  loom-postgresql:
    image: postgres:15-alpine
    container_name: loom-postgresql
    restart: unless-stopped
    environment:
      POSTGRES_USER: loom
      POSTGRES_PASSWORD: loom
      POSTGRES_DB: loom
    networks:
      - loom-network
    volumes:
      - loom-db-postgres:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U loom"]
      interval: 10s
      timeout: 5s
      retries: 5
    ports:
      - "5432:5432"  # Expose for development/debugging

  # TokenHub: LLM routing proxy with intelligent provider selection,
  # health tracking, failover, and Thompson Sampling.
  # All Loom agents route LLM requests through TokenHub instead of
  # directly hitting provider endpoints.
  #
  # I start this service only under the "embedded-tokenhub" profile.
  # If you already run a standalone TokenHub, I skip it — the Makefile
  # probes localhost:8090/healthz before choosing which mode to use.
  #
  # Secrets (TOKENHUB_VAULT_PASSWORD, TOKENHUB_ADMIN_TOKEN) live in
  # ~/.loom/config.env — never in the repo. See .env.example for details.
  tokenhub:
    profiles: [embedded-tokenhub]
    build:
      context: ../tokenhub
      dockerfile: Dockerfile
    image: tokenhub:latest
    container_name: tokenhub
    restart: unless-stopped
    env_file:
      - path: ${HOME}/.loom/config.env
        required: false
    environment:
      - TOKENHUB_LISTEN_ADDR=:8080
      - TOKENHUB_LOG_LEVEL=info
      - TOKENHUB_DB_DSN=file:/data/tokenhub.sqlite?_pragma=busy_timeout(5000)&_pragma=journal_mode(WAL)
      # Vault: enabled with headless auto-unlock. Password comes from ~/.loom/config.env.
      - TOKENHUB_VAULT_ENABLED=true
      # Routing policy: balanced selection, generous budget/latency for agent workloads
      - TOKENHUB_DEFAULT_MODE=normal
      - TOKENHUB_DEFAULT_MAX_BUDGET_USD=1.00
      - TOKENHUB_DEFAULT_MAX_LATENCY_MS=120000
      - TOKENHUB_PROVIDER_TIMEOUT_SECS=120
      # Security: TOKENHUB_ADMIN_TOKEN comes from ~/.loom/config.env if set.
      # If unset, tokenhub auto-generates a random token logged at startup.
      - TOKENHUB_CORS_ORIGINS=*
      - TOKENHUB_RATE_LIMIT_RPS=120
      - TOKENHUB_RATE_LIMIT_BURST=240
      - TOKENHUB_TEMPORAL_ENABLED=false
    networks:
      - loom-network
    volumes:
      - tokenhub-data:/data
    ports:
      - "8090:8080"  # TokenHub admin UI on 8090 to avoid conflict with Loom on 8080
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/healthz"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s

  # NATS message bus with JetStream for inter-service communication
  nats:
    image: nats:2.10-alpine
    container_name: nats
    restart: unless-stopped
    command:
      - "-js"
      - "-sd"
      - "/data"
      - "-m"
      - "8222"
    ports:
      - "4222:4222"  # Client connections
      - "8222:8222"  # HTTP management/monitoring
      - "6222:6222"  # Cluster connections
    networks:
      - loom-network
    volumes:
      - nats-data:/data
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8222/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5

  # PgBouncer connection pooler for loom-postgresql
  pgbouncer:
    image: edoburu/pgbouncer:latest
    container_name: pgbouncer
    restart: unless-stopped
    depends_on:
      loom-postgresql:
        condition: service_healthy
    environment:
      - DB_USER=loom
      - DB_PASSWORD=loom
      - DB_HOST=loom-postgresql
      - DB_PORT=5432
      - DB_NAME=loom
      - POOL_MODE=session
      - MAX_CLIENT_CONN=100
      - DEFAULT_POOL_SIZE=20
      - SERVER_RESET_QUERY=DISCARD ALL
      - IGNORE_STARTUP_PARAMETERS=extra_float_digits
    ports:
      - "5433:5432"  # Expose on 5433 to avoid conflict with loom-postgresql:5432
    networks:
      - loom-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost -U loom"]
      interval: 10s
      timeout: 5s
      retries: 5

  loom:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        GITHUB_TOKEN: ${GITHUB_TOKEN:-}
    image: loom:latest
    container_name: loom
    restart: unless-stopped
    depends_on:
      nats:
        condition: service_healthy
      pgbouncer:
        condition: service_healthy
      # NOTE: tokenhub is NOT listed here. It runs under the "embedded-tokenhub"
      # profile and may not be present. Loom's heartbeat loop retries provider
      # health — if tokenhub isn't up yet (or is external), Loom handles it.
    ports:
      - "8081:8081"
      - "3307:3307"
    environment:
      - LOOM_PASSWORD=${LOOM_PASSWORD:-}            # optional: web UI password; blank = no password
      - GITHUB_TOKEN=${GITHUB_TOKEN:-}              # optional: enables GitHub repo access and CI monitor
      - GITLAB_TOKEN=${GITLAB_TOKEN:-}              # optional: enables GitLab repo access
      - TZ=UTC
      - NATS_URL=nats://nats:4222
      - CONFIG_PATH=/app/config.yaml
      - DOLT_PORT=3307
      - DB_TYPE=postgres
      - POSTGRES_HOST=pgbouncer
      - POSTGRES_PORT=5432
      - POSTGRES_USER=loom
      - POSTGRES_PASSWORD=loom
      - POSTGRES_DB=loom
      - CONNECTORS_SERVICE_ADDR=connectors-service:50051
      - OTEL_ENDPOINT=otel-collector:4317
      # LOOM_PROVIDER_URL: any OpenAI-compatible endpoint works here
      # (embedded TokenHub, standalone TokenHub, direct OpenAI, vLLM, etc.)
      # When using an external provider, the Makefile sets this to the host gateway.
      - LOOM_PROVIDER_URL=${LOOM_PROVIDER_URL:-http://tokenhub:8080}
      - LOOM_PROVIDER_API_KEY=${LOOM_PROVIDER_API_KEY:-}  # optional: auth key for LOOM_PROVIDER_URL; blank if using embedded TokenHub without auth
      - TOKENHUB_ADMIN_TOKEN=${TOKENHUB_ADMIN_TOKEN:-}  # optional: blank = TokenHub auto-generates one at startup
    networks:
      - loom-network
    extra_hosts:
      - "plubbit.local:10.11.101.250"
      - "sparky.local:10.11.100.116"
    volumes:
      # Persist SQLite database and project clones across container restarts
      - loom-db:/app/data
      # Mount known_hosts for git SSH operations (read-only)
      # Loom uses its own per-project SSH deploy keys (stored encrypted in DB)
      - ~/.ssh/known_hosts:/home/loom/.ssh/known_hosts:ro
      # Mount Docker socket for per-project container orchestration
      - /var/run/docker.sock:/var/run/docker.sock
    healthcheck:
      test: ["CMD", "/app/loom", "-version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    labels:
      linkerd.io/inject: enabled

  # Agent Service: Coder -- subscribes to loom.tasks.*.coder
  loom-agent-coder:
    build:
      context: .
      dockerfile: Dockerfile
    image: loom:latest
    restart: unless-stopped
    depends_on:
      nats:
        condition: service_healthy
      loom:
        condition: service_healthy
    environment:
      - PROJECT_ID=${PROJECT_ID:-loom}
      - CONTROL_PLANE_URL=http://loom:8081
      - NATS_URL=nats://nats:4222
      - AGENT_ROLE=coder
      - PROVIDER_ENDPOINT=${PROVIDER_ENDPOINT:-}    # required for agents: OpenAI-compatible URL (e.g. http://tokenhub:8080/v1)
      - PROVIDER_MODEL=${PROVIDER_MODEL:-qwen3-235b-a22b}
      - PROVIDER_API_KEY=${PROVIDER_API_KEY:-}      # required for agents: API key for PROVIDER_ENDPOINT
      - ACTION_LOOP_ENABLED=true
      - MAX_LOOP_ITERATIONS=25
      - WORK_DIR=/workspace
      - OTEL_ENDPOINT=otel-collector:4317
    networks:
      - loom-network
    volumes:
      - agent-coder-workspace:/workspace
    entrypoint: ["/app/loom-project-agent"]
    labels:
      linkerd.io/inject: enabled
    deploy:
      replicas: ${CODER_REPLICAS:-1}

  # Agent Service: Reviewer -- subscribes to loom.reviews.*
  loom-agent-reviewer:
    build:
      context: .
      dockerfile: Dockerfile
    image: loom:latest
    restart: unless-stopped
    depends_on:
      nats:
        condition: service_healthy
      loom:
        condition: service_healthy
    environment:
      - PROJECT_ID=${PROJECT_ID:-loom}
      - CONTROL_PLANE_URL=http://loom:8081
      - NATS_URL=nats://nats:4222
      - AGENT_ROLE=reviewer
      - PROVIDER_ENDPOINT=${PROVIDER_ENDPOINT:-}    # required for agents: OpenAI-compatible URL
      - PROVIDER_MODEL=${PROVIDER_MODEL:-qwen3-235b-a22b}
      - PROVIDER_API_KEY=${PROVIDER_API_KEY:-}      # required for agents: API key for PROVIDER_ENDPOINT
      - ACTION_LOOP_ENABLED=true
      - MAX_LOOP_ITERATIONS=15
      - WORK_DIR=/workspace
      - OTEL_ENDPOINT=otel-collector:4317
    networks:
      - loom-network
    volumes:
      - agent-reviewer-workspace:/workspace
    entrypoint: ["/app/loom-project-agent"]
    labels:
      linkerd.io/inject: enabled
    deploy:
      replicas: ${REVIEWER_REPLICAS:-1}

  # Agent Service: QA -- subscribes to loom.tasks.*.qa
  loom-agent-qa:
    build:
      context: .
      dockerfile: Dockerfile
    image: loom:latest
    restart: unless-stopped
    depends_on:
      nats:
        condition: service_healthy
      loom:
        condition: service_healthy
    environment:
      - PROJECT_ID=${PROJECT_ID:-loom}
      - CONTROL_PLANE_URL=http://loom:8081
      - NATS_URL=nats://nats:4222
      - AGENT_ROLE=qa
      - PROVIDER_ENDPOINT=${PROVIDER_ENDPOINT:-}    # required for agents: OpenAI-compatible URL
      - PROVIDER_MODEL=${PROVIDER_MODEL:-qwen3-235b-a22b}
      - PROVIDER_API_KEY=${PROVIDER_API_KEY:-}      # required for agents: API key for PROVIDER_ENDPOINT
      - ACTION_LOOP_ENABLED=true
      - MAX_LOOP_ITERATIONS=10
      - WORK_DIR=/workspace
      - OTEL_ENDPOINT=otel-collector:4317
    networks:
      - loom-network
    volumes:
      - agent-qa-workspace:/workspace
    entrypoint: ["/app/loom-project-agent"]
    labels:
      linkerd.io/inject: enabled
    deploy:
      replicas: ${QA_REPLICAS:-1}

  # Connectors microservice — manages external service integrations via gRPC
  connectors-service:
    build:
      context: .
      dockerfile: Dockerfile
    image: loom:latest
    container_name: connectors-service
    restart: unless-stopped
    depends_on:
      nats:
        condition: service_healthy
    environment:
      - GRPC_PORT=50051
      - CONFIG_PATH=/app/config/connectors.yaml
      - HEALTH_INTERVAL=30s
      - OTEL_ENDPOINT=otel-collector:4317
      - GITHUB_WEBHOOK_SECRET=${GITHUB_WEBHOOK_SECRET:-}  # optional: validates inbound GitHub webhook signatures
      - GITHUB_TOKEN=${GITHUB_TOKEN:-}                     # optional: enables GitHub API access for connectors
    ports:
      - "50051:50051"  # gRPC
      - "8082:8080"    # GitHub webhook receiver
    networks:
      - loom-network
    volumes:
      - ./config:/app/config
    entrypoint: ["/app/connectors-service"]
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://localhost:50051 || true"]
      interval: 15s
      timeout: 5s
      retries: 3
    labels:
      linkerd.io/inject: enabled

  # Agent Service: Public Relations Manager -- monitors GitHub issues and PRs
  loom-agent-pr-manager:
    build:
      context: .
      dockerfile: Dockerfile
    image: loom:latest
    restart: unless-stopped
    depends_on:
      nats:
        condition: service_healthy
      loom:
        condition: service_healthy
    environment:
      - PROJECT_ID=${PROJECT_ID:-loom}
      - CONTROL_PLANE_URL=http://loom:8081
      - NATS_URL=nats://nats:4222
      - AGENT_ROLE=public-relations-manager
      - PROVIDER_ENDPOINT=${PROVIDER_ENDPOINT:-}    # required for agents: OpenAI-compatible URL
      - PROVIDER_MODEL=${PROVIDER_MODEL:-qwen3-235b-a22b}
      - PROVIDER_API_KEY=${PROVIDER_API_KEY:-}      # required for agents: API key for PROVIDER_ENDPOINT
      - ACTION_LOOP_ENABLED=true
      - MAX_LOOP_ITERATIONS=20
      - WORK_DIR=/workspace
      - OTEL_ENDPOINT=otel-collector:4317
      - GITHUB_TOKEN=${GITHUB_TOKEN:-}
    networks:
      - loom-network
    volumes:
      - agent-pr-manager-workspace:/workspace
    entrypoint: ["/app/loom-project-agent"]
    labels:
      linkerd.io/inject: enabled
    deploy:
      replicas: ${PR_MANAGER_REPLICAS:-1}

  loom-test:
    build:
      context: .
      dockerfile: Dockerfile
      target: builder
      args:
        GITHUB_TOKEN: ${GITHUB_TOKEN:-}
    container_name: loom-test
    networks:
      - loom-network
    command: ["go", "test", "-v", "./..."]

  # OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    container_name: otel-collector
    restart: unless-stopped
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./config/otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "8888:8888"   # Prometheus metrics exposed by the collector
      - "8889:8889"   # Prometheus exporter
    networks:
      - loom-network

  # Prometheus for metrics storage
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - loom-network

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - grafana-data:/var/lib/grafana
    ports:
      - "3000:3000"
    networks:
      - loom-network
    depends_on:
      - prometheus

  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    restart: unless-stopped
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "16686:16686"  # Jaeger UI
      - "14268:14268"  # Jaeger collector HTTP
    networks:
      - loom-network

  # Loki for log aggregation
  loki:
    image: grafana/loki:latest
    container_name: loki
    restart: unless-stopped
    command: -config.file=/etc/loki/local-config.yaml
    ports:
      - "3100:3100"  # Loki API
    networks:
      - loom-network
    volumes:
      - ./config/loki:/etc/loki
      - loki-data:/loki

  # Promtail for log forwarding
  promtail:
    image: grafana/promtail:latest
    container_name: promtail
    restart: unless-stopped
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - ./config/promtail:/etc/promtail
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - loom-network
    depends_on:
      - loki

networks:
  loom-network:
    driver: bridge

volumes:
  loom-db:
  loom-db-postgres:
  nats-data:
  prometheus-data:
  grafana-data:
  loki-data:
  tokenhub-data:
  agent-coder-workspace:
  agent-reviewer-workspace:
  agent-qa-workspace:
  agent-pr-manager-workspace:
