# Loom Configuration

server:
  http_port: 8080
  https_port: 8443
  enable_http: true
  enable_https: false
  tls_cert_file: ""  # Path to TLS certificate (when ready)
  tls_key_file: ""   # Path to TLS key (when ready)
  read_timeout: 30s
  write_timeout: 30s
  idle_timeout: 120s

database:
  type: sqlite  # "sqlite" for single instance, "postgres" for distributed deployment
  path: ./loom.db
  
  # For distributed deployment (High Availability):
  # type: postgres
  # dsn: "postgresql://user:password@localhost:5432/loom?sslmode=require"

beads:
  bd_path: bd  # Path to bd executable
  auto_sync: true
  sync_interval: 5m
  compact_old_days: 90  # Compact closed beads older than 90 days

agents:
  max_concurrent: 10
  default_persona_path: ./personas
  heartbeat_interval: 30s
  file_lock_timeout: 10m

dispatch:
  max_hops: 20  # Maximum times a bead can be redispatched before escalation

security:
  enable_auth: true
  pki_enabled: false  # Will be enabled when certificates are provided
  ca_file: ""
  require_https: false
  allowed_origins:
    - "*"  # CORS - adjust in production
  # api_keys:
  #   - "your-api-key-here"

temporal:
  host: localhost:7233
  namespace: loom-default
  task_queue: loom-tasks
  workflow_execution_timeout: 24h
  workflow_task_timeout: 10s
  enable_event_bus: true
  event_buffer_size: 1000

cache:
  enabled: true               # Enable response caching
  backend: memory             # Cache backend: "memory" (default) or "redis"
  default_ttl: 1h             # Default time-to-live for cache entries
  max_size: 10000             # Maximum number of cached entries (memory backend only)
  max_memory_mb: 500          # Maximum memory usage in MB (memory backend only)
  cleanup_period: 5m          # How often to clean expired entries (memory backend only)
  redis_url: ""               # Redis URL (e.g., redis://localhost:6379/0) - required for redis backend

projects:
  - id: loom-self
    name: Loom Self-Improvement
    git_repo: https://github.com/jordanhubbard/Loom
    branch: main
    beads_path: .beads
    is_perpetual: true
    is_sticky: true
    context:
      build_command: "go build"
      test_command: "go test ./..."
      description: "The Loom project itself - perpetual self-improvement"
  - id: example-project
    name: Example Project
    git_repo: /path/to/repo
    branch: main
    beads_path: .beads
    context:
      build_command: "make build"
      test_command: "make test"

web_ui:
  enabled: true
  static_path: ./web/static
  refresh_interval: 5  # seconds

# LLM Provider
# I register providers via the REST API (POST /api/v1/providers) or via bootstrap.local.
# Any OpenAI-compatible endpoint works: TokenHub, OpenAI, Anthropic, vLLM, etc.
#
# Example: embedded TokenHub (default when using 'make start')
#   endpoint: http://localhost:8090/v1
#   api_key:  set via LOOM_PROVIDER_API_KEY in .env
#
# Example: direct OpenAI
#   endpoint: https://api.openai.com/v1
#   api_key:  sk-...
#
# Example: local vLLM server
#   endpoint: http://my-gpu-server:8000/v1
#   api_key:  (empty string or any value)
